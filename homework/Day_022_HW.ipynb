{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in library(cattonum): there is no package called 'cattonum'\n",
     "output_type": "error",
     "traceback": [
      "Error in library(cattonum): there is no package called 'cattonum'\nTraceback:\n",
      "1. library(cattonum)",
      "2. stop(txt, domain = NA)"
     ]
    }
   ],
   "source": [
    "#觀察範例，在房價預測中調整標籤編碼(Label Encoder) / 獨熱編碼 (One Hot Encoder) 方式，\n",
    "#對於線性迴歸以及梯度提升樹兩種模型，何者影響比較大?\n",
    "library(magrittr)\n",
    "library(caret)\n",
    "library(cattonum)\n",
    "\n",
    "train=read.csv('C:\\\\Users\\\\coco40725\\\\Documents\\\\GitHub\\\\2nd-ML100Days\\\\data\\\\house.train.csv',sep=',',header=T)\n",
    "test=read.csv('C:\\\\Users\\\\coco40725\\\\Documents\\\\GitHub\\\\2nd-ML100Days\\\\data\\\\house.test.csv',sep=',',header=T)\n",
    "\n",
    "##extract categorical variable\n",
    "col.type=rep(0,81)\n",
    "for (i in 1:81) {\n",
    "  col.type[i]=class(train[,i])\n",
    "}\n",
    "col.factor=which(col.type=='factor')\n",
    "\n",
    "\n",
    "##only keep categorical column\n",
    "train.cate=train[,c(col.factor,81)]\n",
    "test.cate=test[,col.factor]\n",
    "\n",
    "## covert NA into a level\n",
    "for (j in 1:43) {\n",
    "  train.cate[,j] %<>% addNA(.)\n",
    "  \n",
    "}\n",
    "\n",
    "\n",
    "## linear regression+ lable coding\n",
    "train.cate.label=train.cate\n",
    "for (k in 1:43) {\n",
    "  train.cate.label[,k] %<>% as.numeric(.)\n",
    "}\n",
    "t11=Sys.time()\n",
    "train_control <- trainControl(method=\"cv\", number=5)\n",
    "cv1= train(SalePrice~., data=train.cate.label, trControl=train_control, method=\"lm\")\n",
    "t12=Sys.time()\n",
    "time1=difftime(t11,t12,units='secs')\n",
    "rmse1=cv1$results[2]\n",
    "rsq1=cv1$results[3]\n",
    "fit1=lm(SalePrice~.,data=train.cate.label)\n",
    "summary(fit1)\n",
    "\n",
    "\n",
    "\n",
    "## linear regression + one hot coding\n",
    "\n",
    "train_control <- trainControl(method=\"cv\", number=5)\n",
    "t11=Sys.time()\n",
    "cv2= train(SalePrice~., data=train.cate, trControl=train_control, method=\"lm\")\n",
    "t12=Sys.time()\n",
    "\n",
    "time2=difftime(t11,t12,units='secs')\n",
    "rmse2=cv2$results[2]\n",
    "rsq2=cv2$results[3]\n",
    "fit2=lm(SalePrice~.,data=train.cate)\n",
    "summary(fit2)\n",
    "\n",
    "\n",
    "\n",
    "library(dummies)\n",
    "\n",
    "## gboost + lable coding\n",
    "t11=Sys.time()\n",
    "xgb.fit1= train(SalePrice~., data=train.cate.label, trControl=train_control, method=\"xgbTree\")\n",
    "t12=Sys.time()\n",
    "gb.req1=mean(xgb.fit1$results$Rsquared)\n",
    "gb.rmse1=mean(xgb.fit1$results$RMSE)\n",
    "time.gb1=difftime(t11,t12,units='secs')\n",
    "\n",
    "## gboost + one hot coding\n",
    "train.dummy=catto_dummy(train.cate,colnames(train.cate)[-44])\n",
    "train.dummy=cbind(train.dummy,SalePrice=train.cate[,44])\n",
    "t11=Sys.time()\n",
    "xgb.fit2= train(SalePrice~., data=train.dummy, trControl=train_control, method=\"xgbTree\")\n",
    "t12=Sys.time()\n",
    "gb.req2=mean(xgb.fit2$results$Rsquared)\n",
    "gb.rmse2=mean(xgb.fit2$results$RMSE)\n",
    "time.gb2=difftime(t11,t12,units='secs')\n",
    "\n",
    "######## 解析\n",
    "\n",
    "\n",
    "result.fin=data.frame(type=c('reg+label','reg+one-hot','xgb+label','xgb+one-hot'),\n",
    "                      rsq=as.numeric(c(rsq1,rsq2,gb.req1,gb.req2)),\n",
    "                      rmse=as.numeric(c(rmse1,rmse2,gb.rmse1,gb.rmse2)),\n",
    "                      time=-c(time1,time2,time.gb1,time.gb2))\n",
    "result.fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear regression and xgboost has better performance in one hot coding. However, it is obvious that we need to spend much time calculating cv results, since one hot coding increase a lot of dummy variables.\n",
    "\n",
    "### since our data is nominal variable, it is inappropriate to use label coding in this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in library(cattonum): there is no package called 'cattonum'\n",
     "output_type": "error",
     "traceback": [
      "Error in library(cattonum): there is no package called 'cattonum'\nTraceback:\n",
      "1. library(cattonum)",
      "2. stop(txt, domain = NA)"
     ]
    }
   ],
   "source": [
    "#鐵達尼號例題中，標籤編碼 / 獨熱編碼又分別對預測結果有何影響? (Hint : 參考今日範例)\n",
    "#鐵達尼號例題中，標籤編碼 / 獨熱編碼又分別對預測結果有何影響? (Hint : 參考今日範例)\n",
    "library(magrittr)\n",
    "library(caret)\n",
    "library(cattonum)\n",
    "train=read.csv('C:\\\\Users\\\\coco40725\\\\Documents\\\\GitHub\\\\2nd-ML100Days\\\\data\\\\titanic_train.csv')\n",
    "train=train[,-c(1,4,9)]\n",
    "\n",
    "##extract categorical variable\n",
    "col.type=rep(0,9)\n",
    "for (i in 1:9) {\n",
    "  col.type[i]=class(train[,i])\n",
    "}\n",
    "col.factor=which(col.type=='factor')\n",
    "\n",
    "\n",
    "\n",
    "##only keep categorical column\n",
    "train.cate=train[,c(col.factor,1)]\n",
    "\n",
    "## covert NA into a level\n",
    "for (j in 1:3) {\n",
    "  train.cate[,j] %<>% addNA(.)\n",
    "  \n",
    "}\n",
    "\n",
    "## linear regression+ lable coding\n",
    "train.cate.label=train.cate\n",
    "for (k in 1:3) {\n",
    "  train.cate.label[,k] %<>% as.numeric(.)\n",
    "}\n",
    "\n",
    "\n",
    "t11=Sys.time()\n",
    "train_control <- trainControl(method=\"cv\", number=5)\n",
    "cv1= train(as.factor(Survived)~., data=train.cate.label, family=binomial,trControl=train_control, method=\"glm\")\n",
    "t12=Sys.time()\n",
    "time1=difftime(t11,t12,units='secs')\n",
    "racc1=cv1$results[2]\n",
    "rkappa1=cv1$results[3]\n",
    "\n",
    "## linear regression + one hot coding\n",
    "train_control <- trainControl(method=\"cv\", number=5)\n",
    "t11=Sys.time()\n",
    "cv2= train(as.factor(Survived)~., data=train.cate, family=binomial,trControl=train_control, method=\"glm\")\n",
    "t12=Sys.time()\n",
    "\n",
    "time2=difftime(t11,t12,units='secs')\n",
    "racc2=cv2$results[2]\n",
    "rkappa2=cv2$results[3]\n",
    "\n",
    "\n",
    "\n",
    "## gboost + lable coding\n",
    "t11=Sys.time()\n",
    "xgb.fit1 =train(as.factor(Survived)~., data=train.cate.label, trControl=train_control, method=\"xgbTree\")\n",
    "t12=Sys.time()\n",
    "gb.acc1=mean(xgb.fit1$results$Accuracy)\n",
    "gb.kappa1=mean(xgb.fit1$results$Kappa)\n",
    "time.gb1=difftime(t11,t12,units='secs')\n",
    "\n",
    "## gboost + one hot coding\n",
    "train.dummy=catto_dummy(train.cate,colnames(train.cate)[-4])\n",
    "train.dummy=cbind(train.dummy,Survived=train.cate[,4])\n",
    "t11=Sys.time()\n",
    "xgb.fit2= train(as.factor(Survived)~., data=train.dummy, trControl=train_control, method=\"xgbTree\")\n",
    "t12=Sys.time()\n",
    "gb.acc2=mean(xgb.fit2$results$Accuracy)\n",
    "gb.kappa2=mean(xgb.fit2$results$Kappa)\n",
    "time.gb2=difftime(t11,t12,units='secs')\n",
    "\n",
    "\n",
    "result.fin=data.frame(type=c('reg+label','reg+one-hot','xgb+label','xgb+one-hot'),\n",
    "                      acc=as.numeric(c(racc1,racc2,gb.acc1,gb.acc2)),\n",
    "                      kappa=as.numeric(c(rkappa1,rkappa2,gb.kappa1,gb.kappa2)),\n",
    "                      time=-c(time1,time2,time.gb1,time.gb2))\n",
    "result.fin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In titanic data which is a calssification problem,  logistic and xgboost still have slightly better performance in one hot coding, and still, one hot coding is more time consumming than label coding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
